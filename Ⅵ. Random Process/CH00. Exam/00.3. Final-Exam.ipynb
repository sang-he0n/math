{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2ab880f",
   "metadata": {},
   "source": [
    "# CH00.3. **Final-term Examination**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee4e39d",
   "metadata": {},
   "source": [
    "> ## **Q1. Compound Poisson Process**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5399767",
   "metadata": {},
   "source": [
    "#### $ \\text{Let } J_{1}, J_{2}, \\cdots{} \\text{ be independent and identically distributed with zero mean}, \\text{ variance } \\, \\sigma{}_{J}^{2} $ \n",
    "#### $ \\text{and characteristic function } \\varPhi_{J}(u), \\; \\text{and let } ( N_{t} : t \\ge{} 0 ) \\text{ be a Poisson counting process with rate } \\lambda{} > 0, $ \n",
    "#### $ \\text{which is statistically independent of the } J\\text{'s}. \\; \\text{Define } Y_{t} \\, \\text{ for } \\, t \\ge{} 0 \\, \\text{ by } \\, Y_{t} = \\displaystyle{} \\sum_{i=1}^{N_{t}} J_{i}. $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6572345",
   "metadata": {},
   "source": [
    "#### **(a)** $ \\text{Show that } \\mathbb{E}[ Y_{t} ] = 0 \\, \\text{ for } \\, t \\ge{} 0 $\n",
    "##### $ \\text{(Hint: } \\mathbb{E}[ Y_{t} ] \\, \\text{ is the average of } \\, \\mathbb{E}[ Y_{t} \\mid{} N_{t} = n ] \\, \\text{ over } \\, n \\, \\text{ using the pmf of } N_{t} \\text{.)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd290972",
   "metadata": {},
   "source": [
    "#### **(b)** $ \\text{Briefly explain why } Y \\text{ has independent increments.} $ \n",
    "##### $ \\big({}\\text{Note: (a), (b) and the fact } \\, Y_{0} = 0 \\, \\text{ imply } ( Y_{t} : t \\ge{} 0 ) \\, \\text{ is a martingale.} \\big){} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f96408",
   "metadata": {},
   "source": [
    "#### **(c)** $ \\text{Express } \\, \\mathbb{E}[ Y_{t}^{2} ] \\, \\text{ and the characteristic function } \\, \\mathbb{E}[ e^{ j u Y_{t} } ] \\, \\text{ for } \\, t \\ge{} 0 \\text{ in terms of } \\, \\lambda{}, \\; \\sigma{}_{J}^{2} \\, \\text{ and } \\, \\varPhi{}_{J}. $\n",
    "##### $ \\text{(Hint: See hint for part (a))} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25be0cc",
   "metadata": {},
   "source": [
    "#### **(d)** $ \\text{Show that } ( Z_{t} : t \\ge{} 0 ) \\text{ defined by } Z_{t} = Y_{t}^{2} - c t \\, \\text{ is a martingale for some constant } \\, c, \\text{ and identify } \\, c. $\n",
    "##### $ \\text{(Hint: For } s < t, \\; Y_{t} = Y_{s} + ( Y_{t} - Y_{s} ) \\text{. It suffices to show that } \\mathbb{E}[ Z_{ t_{n+1} } \\mid{} Y_{ t_{1} }, \\cdots{}, Y_{ t_{n} } ] = Z_{ t_{n} } \\text{ whenever } t_{1} < \\cdots < t_{n+1}.) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd983b0",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c344e14d",
   "metadata": {},
   "source": [
    "> ## **Q2. Brownian Bridge Process**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f5823a",
   "metadata": {},
   "source": [
    "#### $ \\text{Let } W = ( W_{t} : t \\ge{} 0 ) \\text{ be a standard Brownian motion } \\text{(i.e. a Brownian motion with parameter } \\sigma{}^{2} = 1 \\text{)}. $\n",
    "#### $ \\text{Let } B_{t} = W_{t} - t W_{1} \\, \\text{ for } \\, 0 \\le{} t \\le{} 1. \\; \\text{The process } \\, B = ( B_{t} : 0 \\le{} t \\le{} 1 ) \\, \\text{ is called a Brownian bridge process.} $\n",
    "#### $ \\text{Like } W, B \\, \\text{ is a mean zero Gaussian random process.} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c240dc0",
   "metadata": {},
   "source": [
    "#### **(a)** $ \\text{Sketch a typical sample path of } \\, W \\text{ and the corresponding sample path of } \\, B. $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd972eac",
   "metadata": {},
   "source": [
    "#### **(b)** $ \\text{Find the autocorrelation function of } \\, B. $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db4f17e",
   "metadata": {},
   "source": [
    "#### **(c)** $ \\text{Is } B \\text{ a Markov process?} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b3e6e2",
   "metadata": {},
   "source": [
    "#### **(d)** $ \\text{Show that } B \\text{ is independent of the random variable } W_{1}. $\n",
    "##### $ \\text{(This means that for any finite collection } t_{1}, \\cdots{}, t_{n} \\in{} [ 0, 1 ] \\text{, the random vector } ( B_{ t_{1} }, \\cdots{}, B_{ t_{n} } )^{T} \\text{ is independent of } W_{1} \\text{.)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2e4890",
   "metadata": {},
   "source": [
    "#### **(e)** $ \\big({} \\text{Due to J. L. Doob.} \\big){} \\text{ Let } X_{t} = ( 1 - t ) W_{ \\frac{ t }{ 1 - t } } \\, \\text{ for } \\, 0 \\le{} t < 1. \\text{ Let } X_{1} = 0. $\n",
    "#### $ \\text{Let } X \\text{ denote the random process } \\, X = ( X_{t} : 0 \\le{} t \\le{} 1 ). $\n",
    "#### $ \\text{Like } W, X \\text{ is a mean zero Gaussian random process. Find the autocorrelation function of } \\, X. $\n",
    "#### $ \\text{Can you draw any conclusions?} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d402f9",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bb0210",
   "metadata": {},
   "source": [
    "> ## **Q3. Conditional Distribution of Gaussian**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfacd123",
   "metadata": {},
   "source": [
    "#### $ \\text{Let } X \\text{ and } Y \\text{ be jointly Gaussian random variables with zero mean}, $\n",
    "#### $ \\text{such that the vector } \\begin{bmatrix} X \\\\ Y \\end{bmatrix} \\text{ has covariance matrix } \\begin{bmatrix} 4 & 4 \\\\ 4 & 8 \\end{bmatrix}. $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7531bcf",
   "metadata": {},
   "source": [
    "#### **(a)** $ \\text{Find the conditional expectation } \\, \\mathbb{E}{[ e^{t X} \\mid{} Y ]} $\n",
    "#### $ (\\text{the answer must be a function of } \\, Y \\text{ and } \\, t.) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090b3ba5",
   "metadata": {},
   "source": [
    "#### **(b)** $ \\text{Express the conditional probability } \\mathbb{P}{( | X | \\ge{} 4 \\mid{} Y )} \\text{ in terms of the standard Gaussian cdf } \\, \\varPhi{( u )} $ \n",
    "##### $ (\\text{Note} : \\varPhi{( u )} = \\frac{ 1 }{ \\sqrt{ 2 \\pi{} } } \\int_{ -\\infty{} }^{u} e^{ - x^{2} / 2 } \\, \\mathrm{d} x) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93797fd8",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daad286c",
   "metadata": {},
   "source": [
    "> ## **Q4. Poisson Counting Process**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679d813a",
   "metadata": {},
   "source": [
    "#### $ \\text{Let } N = ( N_{t} : t \\ge{} 0 ) \\text{ be a Poisson counting process with rate } \\lambda{} > 0. $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8c13c9",
   "metadata": {},
   "source": [
    "#### **(a)** $ \\text{Give a simple expression for } \\, \\mathbb{P}{( N_{1} \\geq{} 1 \\mid{} N_{2} = 2 )} \\text{ in terms of } \\, \\lambda{}. $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111b69b3",
   "metadata": {},
   "source": [
    "#### **(b)** $ \\text{Give a simple expression for } \\, \\mathbb{P}{( N_{2} = 2 \\mid{} N_{1} \\geq{} 1 )} \\text{ in terms of } \\, \\lambda{}. $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ef2234",
   "metadata": {},
   "source": [
    "#### **(c)** $ \\text{Let } X_{t} = N_{t}^{2}. \\, \\text{ Is } X = ( X_{t} : t \\ge{} 0 ) \\, \\text{ a time-homogeneous Markov process?} $\n",
    "#### $ \\text{If so, give the transition probabilities } \\, p_{ i j }( \\tau{} ) \\text{. If not, explain.} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cb6767",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff7d218",
   "metadata": {},
   "source": [
    "> ## **Q5-1. Kalman Filter for Range-Tracking Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225fd4b7",
   "metadata": {},
   "source": [
    "#### $ \\text{Consider} $\n",
    "#### $$ x_{ k+1 } = \\begin{bmatrix} 1 & \\Delta{} t & \\tfrac{ 1 }{ 2 } \\Delta{} t^{2} \\\\ 0 & 0 & \\Delta{} t \\\\ 0 & 0 & 1 \\end{bmatrix} x_{ k } + \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix} w_{ k } $$\n",
    "#### $$ y_{ k } = \\begin{bmatrix} 1 & 0 & 0 \\end{bmatrix} x_{ k } + v_{ k } $$\n",
    "#### $ \\text{where } \\{ w_{ k } \\} \\text{ and } \\{ v_{ k } \\} \\text{ are i.i.d. Gaussian random processes, which are independent of each other.} \\, \\text{ Here } x_{ k } = \\begin{bmatrix} x_{ k }^{(1)} \\\\ x_{ k }^{(2)} \\\\ x_{ k }^{(3)} \\end{bmatrix} $\n",
    "#### $ \\text{You may use any software, choose any } \\Delta{} t \\le{} 0.01, \\, \\text{ any noise/initial statistics.} $\n",
    "##### $ (\\text{The measurement data for } \\{ y_k \\} \\text{ is in ``Range.csv''. column1 : time, column2 : value}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0a3122",
   "metadata": {},
   "source": [
    "#### **(a)** $ \\text{Construct the Kalman filter to estimate the data.} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f8152",
   "metadata": {},
   "source": [
    "#### **(b)** $ \\text{Plot your estimation.} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cd5d5a",
   "metadata": {},
   "source": [
    "#### **(c)** $ \\text{Plot } \\lvert{} x_{k}^{(1)} - \\hat{x}_{k}^{(1)} \\rvert{}^{2}, \\text{ i.e. empirical mean-square error.} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9258504",
   "metadata": {},
   "source": [
    "#### **(d)** $ \\text{Discuss the convergence of the error covariance matrix.} $\n",
    "#### $ \\text{And you have to plot the error covariance matrix} $ \n",
    "##### $ \\text{(x-axis: time, y-axis: value of error covariance matrix (each component)).} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd3766d",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc17e7a",
   "metadata": {},
   "source": [
    "> ## **Q5-2. Scalar Kalman Filter Steady State**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73275512",
   "metadata": {},
   "source": [
    "#### $ \\text{Consider the following oneâ€“dimensional Kalman filter model} : $\n",
    "#### $$ x_{ k+1 } = f\\, x_{ k } + w_{ k }, y_{ k } = x_{ k } + v_{ k }, $$\n",
    "#### $ \\text{where } w_{ k } \\, \\text{ and } \\, v_{ k } \\, \\text{ are mutually independent Gaussian random variables with } \\, w_{ k }, v_{ k } \\sim N( 0, 1 ). $\n",
    "#### $ \\text{Assume the prior } x_{0} \\sim N( 0, \\sigma{}_{0}^{2} ). $\n",
    "#### $ \\text{From the standard Kalman filter equations, the estimation error variance } \\sigma{}_{k}^{2} = \\mathrm{Var}( x_{ k } - \\hat{x}_{k \\mid{} k-1} ) \\text{ satisfies} $\n",
    "#### $$ \\sigma{}_{k+1}^{2} = f^{2} \\sigma{}_{k}^{2} + 1 - \\frac{ f^{2} ( \\sigma{}_{k}^{2} )^{2} }{ \\sigma{}_{k}^{2} + 1 } $$\n",
    "#### $ \\text{and the Kalman gain is given by} $\n",
    "#### $$ K_{ k } = \\frac{ f\\, \\sigma{}_{k}^{2} }{ f^{2} \\sigma{}_{k}^{2} + 1 } . $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2962c195",
   "metadata": {},
   "source": [
    "#### **(a)** $ \\text{Show that} \\displaystyle{} \\lim_{ k \\to \\infty{} } \\sigma_{k}^{2} \\, \\text{ exists.} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40d3f2f",
   "metadata": {},
   "source": [
    "#### **(b)** $ \\text{Express the limit } \\, \\sigma{}_{\\infty{}}^{2}, \\text{ in terms of } f. $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c9ca48",
   "metadata": {},
   "source": [
    "#### **(c)** $ \\text{Explain why } \\sigma{}_{\\infty{}}^{2} = 1 \\text{ if } f = 0. $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a2bde9",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07d1867",
   "metadata": {},
   "source": [
    "> ## **Q6. Conditional Independence**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d57b2cb",
   "metadata": {},
   "source": [
    "#### $ \\text{Let } X, Y \\text{ and } Z \\text{ be three random variables with finite second moments, } $\n",
    "#### $ \\text{defined on a common probability space } ( \\Omega, \\mathcal{F}, P ). \\, \\text{ We say that } X \\text{ and } Z \\text{ are conditionally independent given } Y $ \n",
    "#### $ \\text{(and write } X \\to{} Y \\to{} Z ) \\, \\text{ if for any three events } A, B, C \\in \\mathcal{F}, $\n",
    "#### $$ P( X \\in A, Z \\in C \\mid{} Y \\in B ) = P( X \\in A \\mid{} Y \\in B ) P( Z \\in C \\mid{} Y \\in B ). $$\n",
    "#### $ \\text{If } X, Y \\text{ and } Z \\text{ have a joint pdf } \\, f_{ X Y Z }, \\text{ then conditional independence is equivalent to{}} $\n",
    "#### $$ f_{ X Z \\mid{} Y } ( x, z \\mid{} y ) = f_{ X \\mid{} Y } ( x \\mid{} y ) f_{ Z \\mid{} Y } ( z \\mid{} y ). $$\n",
    "#### $ \\text{When working on this problem, you may assume that all necessary joint pdf's exist.} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e06b0",
   "metadata": {},
   "source": [
    "#### **(a)** $ \\text{Given } X, \\text{ let } Y \\text{ and } Z \\text{ be defined by } Y = X + W_{1} \\text{ and } Z = Y + W_{2}, $\n",
    "#### $ \\text{where } X, W_{1} \\text{ and } W_{2} \\text{ are mutually independent random variables. Prove that } X \\to{} Y \\to{} Z. $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e71665c",
   "metadata": {},
   "source": [
    "#### **(b)** $ \\text{Show that } X \\to{} Y \\to{} Z \\, \\text{ implies that } \\, \\mathbb{E}[ X \\mid{} Y, Z ] = \\mathbb{E}[ X \\mid{} Y ]. $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ad753b",
   "metadata": {},
   "source": [
    "#### **(c)** $ \\text{Show that if } X \\to{} Y \\to{} Z, \\text{ then } \\, \\text{MMSE}( X \\mid{} Z ) - \\text{MMSE}( X \\mid{} Y ) = \\mathbb{E}\\big[ ( \\mathbb{E}[ X \\mid{} Y ] - \\mathbb{E}[ X \\mid{} Z ] )^{2} \\big]. $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900d0bb2",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37a4265",
   "metadata": {},
   "source": [
    "> ## **Q7. Orthogonality Principle in $ \\, L^{2} $ space**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79415314",
   "metadata": {},
   "source": [
    "#### $ \\text{Prove Theorem } 3.2. \\text{ (a), (b), (c)}. $\n",
    "##### $ (\\text{Do not copy the proof from the textbook. You should write your own proof.}) $\n",
    "\n",
    "$ \\hspace{0.3cm} \\textbf{THEOREM } 3.2 \\; \\text{(The orthogonality principle)} $\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Let } V \\text{ be a closed, linear subspace of } L^{2}( \\Omega, \\mathcal{F}, P ) \\text{, and let } X \\in L^{2}( \\Omega, \\mathcal{F}, P ) \\text{, for some probability space } ( \\Omega, \\mathcal{F}, P ). $\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{(a) (Existence and uniqueness) There exists a unique element } Z^{*} \\text{ in } V \\text{ so that } \\mathbb{E}[ ( X - Z^{*} )^{2} ] \\le{} \\mathbb{E}[ ( X - Z )^{2} ] \\text{ for all } Z \\in V. $\n",
    "\n",
    "$ \\hspace{0.3cm} \\big({} \\text{Here, we consider two elements } Z \\text{ and } Z' \\text{ of } V \\text{ to be the same if } P \\{ Z = Z' \\} = 1. \\; \\text{also } Z^{*} \\text{ denoted by } \\Pi_{ V }( X ) \\big){} $\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{(b) (Characterization) Let } W \\text{ be a random variable. Then } W = Z^{*} \\text{ if and only if the following two conditions hold:} $\n",
    "\n",
    "$ \\hspace{0.75cm} \\text{(i) } W \\in V $\n",
    "\n",
    "$ \\hspace{0.75cm} \\text{(ii) } ( X - W ) \\perp Z \\text{ for all } Z \\in V. $\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{(c) (Error expression) The minimum mean square error (MMSE) is given by } \\mathbb{E}[ ( X - Z^{*} )^{2} ] = \\mathbb{E}[ X^{2} ] - \\mathbb{E}[ ( Z^{*} )^{2} ]. $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e304f53c",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41613cd9",
   "metadata": {},
   "source": [
    "> ## **Q8-1. Linearity of Orthogonal Projection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934bd5e4",
   "metadata": {},
   "source": [
    "#### $ \\text{Prove Proposition } 3.3 \\text{ of the textbook. } $\n",
    "##### $ (\\text{Do not copy the proof from the textbook. You should write your own proof.}) $\n",
    "\n",
    "$ \\hspace{0.3cm}\\textbf{PROPOSITION } 3.3 \\; \\text{(Linearity of projection)} $\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Suppose } \\mathcal{V} \\text{ is a closed linear subspace of } L^{2}( \\Omega{}, \\mathcal{F}, P ). $ \n",
    "\n",
    "$ \\hspace{0.3cm} X_{ 1 } \\text{ and } X_{ 2 } \\text{ are in } L^{2}( \\Omega{}, \\mathcal{F}, P ) \\text{, and } a_{ 1 } \\text{ and } a_{ 2 } \\text{ are constants. Then} $\n",
    "\n",
    "$$ \\Pi{}_{ \\mathcal{V} }( a_{ 1 } X_{ 1 } + a_{ 2 } X_{ 2 } ) = a_{ 1 } \\Pi_{ \\mathcal{V} }( X_{ 1 } ) + a_{ 2 } \\Pi_{ \\mathcal{V} }( X_{ 2 } ) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e021650",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287fd04a",
   "metadata": {},
   "source": [
    "> ## **Q8-2. Projection onto Nested Subspaces**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0207f2a",
   "metadata": {},
   "source": [
    "#### $ \\text{Prove Proposition } 3.4 \\text{ of the textbook. } $\n",
    "##### $ (\\text{Do not copy the proof from the textbook. You should write your own proof.}) $\n",
    "\n",
    "$ \\hspace{0.3cm}\\textbf{PROPOSITION } 3.4 \\; \\text{(Projections onto nested subspaces)} $\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Suppose } \\mathcal{V}_{1} \\, \\text{ and } \\, \\mathcal{V}_{2} \\, \\text{ are closed linear subspaces of } \\, L^{2}( \\Omega{}, \\mathcal{F}, P ) \\, \\text{ such that } \\, \\mathcal{V}_{2} \\subset{} \\mathcal{V}_{1}. $\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Then for any } X \\in{} L^{2}( \\Omega{}, \\mathcal{F}, P ), \\; \\Pi{}_{ \\mathcal{V}_{2} }( X ) = \\Pi{}_{ \\mathcal{V}_{2} } \\Pi{}_{ \\mathcal{V}_{1} }( X ). \\, \\text{ Furthermore,} $\n",
    "\n",
    "$$ \\mathbb{E}\\left[{} ( X - \\Pi{}_{ \\mathcal{V}_{2} }( X ) )^{2} \\right]{} = \\mathbb{E}\\left[{} ( X - \\Pi{}_{ \\mathcal{V}_{1} }( X ) )^{2} \\right]{} + \\mathbb{E}\\left[{} ( \\Pi{}_{ \\mathcal{V}_{1} }( X ) - \\Pi{}_{ \\mathcal{V}_{2} }( X ) )^{2} \\right]{} $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{In particular, } \\, \\mathbb{E}\\left[{} ( X - \\Pi{}_{ \\mathcal{V}_{2} }( X ) )^{2} \\right]{} \\ge{} \\mathbb{E}\\left[{} ( X - \\Pi{}_{ \\mathcal{V}_{1} }( X ) )^{2} \\right]{}. $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5ca9dd",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4332bf8c",
   "metadata": {},
   "source": [
    "> ## **Q8-3. Projection onto Orthogonal Sum**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb7697e",
   "metadata": {},
   "source": [
    "#### $ \\text{Prove Proposition } 3.5 \\text{ of the textbook. } $\n",
    "##### $ (\\text{Do not copy the proof from the textbook. You should write your own proof.}) $\n",
    "\n",
    "$ \\hspace{0.3cm}\\textbf{PROPOSITION } 3.5 \\; \\text{(Projection onto the span of orthogonal subspaces)} $\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Suppose } \\mathcal{V}_{1} \\text{ and } \\mathcal{V}_{2} \\text{ are closed linear subspaces of } \\, L^{2}( \\Omega{}, \\mathcal{F}, P ) \\, \\text{ such that } \\mathcal{V}_{1} \\perp{} \\mathcal{V}_{2}, \\text{ which means } \\, \\mathbb{E}[ Z_{ 1 } Z_{ 2 } ] = 0 \\text{ for any } Z_{ 1 } \\in{} \\mathcal{V}_{1} \\, \\text{ and } \\, Z_{ 2 } \\in{} \\mathcal{V}_{2}. $\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Let } \\mathcal{V} = \\mathcal{V}_{1} \\oplus{} \\mathcal{V}_{2} = \\{ Z_{ 1 } + Z_{ 2 } : Z_{ i } \\in{} \\mathcal{V}_{i} \\} \\text{ denote the span of } \\mathcal{V}_{1} \\text{ and } \\mathcal{V}_{2}. \\, \\text{ Then for any } X \\in{} L^{2}( \\Omega{}, \\mathcal{F}, P ), \\; \\Pi{}_{ \\mathcal{V} }( X ) = \\Pi{}_{ \\mathcal{V}_{1} }( X ) + \\Pi{}_{ \\mathcal{V}_{2} }( X ). $\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{The minimum mean square error satisfies}: $\n",
    "\n",
    "$$ \\mathbb{E}\\left[ ( X - \\Pi{}_{ \\mathcal{V} }( X ) )^{2} \\right] = \\mathbb{E}[ X^{2} ] - \\mathbb{E}\\left[ ( \\Pi{}_{ \\mathcal{V}_{1} }( X ) )^{2} \\right] - \\mathbb{E}\\left[ ( \\Pi{}_{ \\mathcal{V}_{2} }( X ) )^{2} \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196cc742",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814e1fdc",
   "metadata": {},
   "source": [
    "> ## **Q9. Bernoulli Renewal Age Process**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107d2ec9",
   "metadata": {},
   "source": [
    "#### $ \\text{Let } U = ( U_{k} : k \\in{} \\mathbb{Z} ) \\text{ be such that for some } p \\in{} ( 0, 1 ) \\text{ the random variables } U_{k} \\text{ are independent, } $ \n",
    "#### $ \\text{with each having the Bernoulli distribution with parameter } p \\text{. Interpret } U_{k} = 1 \\text{ to mean that} $ \n",
    "#### $ \\text{a renewal or replacement of some part takes place at time } k. \\, \\text{ For } k \\in{} \\mathbb{Z}, \\, \\text{ let} \\, X_{k} = \\min{} \\{ i \\ge{} 1 : U_{ k - i } = 1 \\} . $\n",
    "#### $ \\text{In words, } X_{k} \\, \\text{ is the time elapsed since the last renewal strictly before time } k. $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100e63ed",
   "metadata": {},
   "source": [
    "#### **(a)** $ \\text{The process } X \\text{ is a time-homogeneous Markov process. Indicate a suitable state space,} $ \n",
    "#### $ \\text{and describe the one-step transition probabilities.} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49effb6",
   "metadata": {},
   "source": [
    "#### **(b)** $ \\text{Find the distribution of } \\, X_{k} \\text{ for } k \\text{ fixed.} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67974f0",
   "metadata": {},
   "source": [
    "#### **(c)** $ \\text{Is } X \\text{ a stationary random process? Explain.} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4cb4d2",
   "metadata": {},
   "source": [
    "#### **(d)** $ \\text{Find the } k\\text{-step transition probabilities } \\, p_{ i j }( k ) = \\mathbb{P} \\{ X_{ n + k } = j \\mid{} X_{ n } = i \\}. $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc73bc8",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c605b259",
   "metadata": {},
   "source": [
    "> ## **Q10. Jump Chain and Sojourn Times**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca36f37",
   "metadata": {},
   "source": [
    "#### $ \\text{Prove Proposition } 4.9 \\text{ of the textbook. } $\n",
    "##### $ (\\text{Do not copy the proof from the textbook. You should write your own proof.}) $\n",
    "\n",
    "$ \\hspace{0.3cm} \\textbf{PROPOSITION } 4.9 \\; \\text{Let } X = ( X( k ) : k \\in{} \\mathbb{Z}_{+} ) \\text{ be a time-homogeneous Markov process with one-step transition probability matrix } P. $\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{(a) The jump process } X^{J} \\text{ is itself a time-homogeneous Markov process, and its one-step transition probabilities are given by } \\, p_{ i j }^{J} = \\dfrac{ p_{ i j } }{ 1 - p_{ i i } } $\n",
    "\n",
    "$ \\hspace{0.75cm} \\text{ for } \\, i \\neq{} j \\, \\text{ and } \\, p_{ i i }^{J} = 0, \\; i, j \\in{} S. $\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{(b) Given } X( 0 ), \\; X^{J}( 1 ) \\, \\text{ is conditionally independent of } \\, T_{0}. $\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{(c) Given } ( X^{J}( 0 ), \\cdots{}, X^{J}( n ) ) = ( j_{0}, \\cdots{}, j_{n} ), \\text{ the variables } \\, T_{0}, \\cdots{}, T_{n} \\, \\text{ are conditionally independent, } $\n",
    "\n",
    "$ \\hspace{0.75cm} \\text{and the conditional distribution of } \\, T_{ l } \\text{ is geometric with parameter } \\, p_{j_{l} j_{l} } : $\n",
    "\n",
    "$$ \\hspace{0.3cm} \\mathbb{P}( T_{ l } = k \\mid{} X^{J}( 0 ) = j_{0}, \\cdots{}, X^{J}( n ) = j_{n} ) = p_{j_{l}j_{l}}^{k - 1} ( 1 - p_{ j_{ l } j_{ l } } ), \\;\\; 0 \\le{} l \\le{} n, \\;\\; k \\ge{} 1. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4737531",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01521e7",
   "metadata": {},
   "source": [
    "> ## **Q11. Simulation of a $3$-State Markov Chain**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b43bb1e",
   "metadata": {},
   "source": [
    "#### $ \\text{Consider the following Markov chain} $\n",
    "#### <p align=\"center\"> <img src=\"../img/00.3. Final-Exam (1).png\" width=\"35%\" height=\"35%\"></img> </p>\n",
    "#### $ \\text{Use MATLAB or Python to solve the following problem} $\n",
    "#### $ \\text{With time sample } 0.01 \\text{, generate a sample path of the Markov chain from time } 0 \\text{ to } 2 $\n",
    "#### $ \\text{Compute the following using MATLAB or Python (for any } k \\ge{} 0 ), $\n",
    "#### $ \\text{and try to compare them with analytic results (theoretical computation).} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f80371d",
   "metadata": {},
   "source": [
    "#### **(a)** $ \\mathbb{P}( X_{ k+1 } = A \\mid{} X_{ k } = B ) $\n",
    "##### $ \\text{(probability that the chain visits state } A \\text{ given current state } B ) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f69d03e",
   "metadata": {},
   "source": [
    "#### **(b)** $ \\mathbb{P}( X_{ k+1 } = A \\mid{} X_{ k } = B, X_{ k-1 } = C ) $\n",
    "##### $ \\text{(probability that the chain visits state } A \\text{ given current state } B \\text{ and previous state } C ) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdba6a2",
   "metadata": {},
   "source": [
    "#### **(c)** $ \\mathbb{P}( X_{ k+1 } = A \\mid{} X_{ k } = B, X_{ k-1 } = C, X_{ k-2 } = B ) $\n",
    "##### $ \\text{(probability that the chain visits state } A \\text{ given current state } B \\text{ and previous states } C \\text{ and } B ) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a829ce3",
   "metadata": {},
   "source": [
    "#### **(d)** $ \\mathbb{E}[ X_{ k+1 } = A \\mid{} X_{ k } = B ] $\n",
    "##### $ \\text{(expected value that the chain visits state } A \\text{ given current state } B ) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad1b612",
   "metadata": {},
   "source": [
    "#### **(e)** $ \\mathbb{E}[ X_{ k+1 } = A \\mid{} X_{ k } = B, X_{ k-1 } = C ] $\n",
    "##### $ \\text{(expected value that the chain visits state } A \\text{ given current state } B \\text{ and previous state } C ) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b67d70",
   "metadata": {},
   "source": [
    "#### **(f)** $ \\mathbb{E}[ X_{ k+1 } = A \\mid{} X_{ k } = B, X_{ k-1 } = C, X_{ k-2 } = B ] $\n",
    "##### $ \\text{(expected value that the chain visits state } A \\text{ given current state } B \\text{ and previous states } C \\text{ and } B ) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ad8a89",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b5726a",
   "metadata": {},
   "source": [
    "> ## **Q12. Nonlinear Transforms: Markov, Stationarity, Martingale**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912a0d3b",
   "metadata": {},
   "source": [
    "#### $ \\text{Let } X = ( X_{ n } : n \\in \\mathbb{Z} ), \\; Y = ( Y_{ n } : n \\in \\mathbb{Z} ), \\; Z = ( Z_{ n } : n \\in \\mathbb{Z} ) \\text{ be random processes such that } $ \n",
    "#### $ Y_{ n } = X_{ n }^{2} \\, \\text{ for all } \\, n \\, \\text{ and } \\, Z_{ n } = X_{ n }^{3} \\, \\text{ for all } \\, n. $\n",
    "#### $ \\text{Determine whether each of the following statements is always true. If true, give a justification.} $ \n",
    "#### $ \\text{If not, give a simple counter example.} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d404ee6",
   "metadata": {},
   "source": [
    "#### **(a)** $ \\text{If } X \\text{ is Markov then } Y \\text{ is Markov.} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6db10f8",
   "metadata": {},
   "source": [
    "#### **(b)** $ \\text{If } X \\text{ is Markov then } Z \\text{ is Markov.} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62fd948",
   "metadata": {},
   "source": [
    "#### **(c)** $ \\text{If } Y \\text{ is Markov then } X \\text{ is Markov.} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18197853",
   "metadata": {},
   "source": [
    "#### **(d)** $ \\text{If } X \\text{ is stationary then } Y \\text{ is stationary.} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4205baf3",
   "metadata": {},
   "source": [
    "#### **(e)** $ \\text{If } Y \\text{ is stationary then } X \\text{ is stationary.} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b53d497",
   "metadata": {},
   "source": [
    "#### **(f)** $ \\text{If } X \\text{ is wide sense stationary then } Y \\text{ is wide sense stationary.} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e549218",
   "metadata": {},
   "source": [
    "#### **(g)** $ \\text{If } X \\text{ has independent increments then } Y \\text{ has independent increments.} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f21c94a",
   "metadata": {},
   "source": [
    "#### **(h)** $ \\text{If } X \\text{ is a martingale then } Z \\text{ is a martingale.} $"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PYTCH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
