{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27de5a30",
   "metadata": {},
   "source": [
    "# CH00.6. **Project Soltuion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712f3a6c",
   "metadata": {},
   "source": [
    "> ## Q1. **Characterizations of Convergence in Distribution**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c0f834",
   "metadata": {},
   "source": [
    "#### $ \\text{Prove Proposition } 2.6. $\n",
    "$ \\hspace{0.3cm}\\textbf{PROPOSITION } 2.6. \\text{(Characterizations of Convergence in Distribution)} $\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Let } (X_{n}) \\text{ be a sequence of random variables and let } X \\text{ be a random variable.} $ \n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Then the following are equivalent}: $\n",
    "\n",
    "$ \\hspace{0.3cm} (ⅰ) \\, X_{n} \\xrightarrow{d} X$  \n",
    "\n",
    "$ \\hspace{0.3cm} (ⅱ) \\, \\mathbb{E}{}\\big[{} f(X_{n}) \\big]{} \\to{} \\mathbb{E}{}\\big[{} f(X) \\big]{} \\text{ for any bounded continuous function } f $.  \n",
    "\n",
    "$ \\hspace{0.3cm} (ⅲ) \\, \\Phi{}_{X_{n}}(u) \\to{} \\Phi{}_{X}(u) \\text{ for each } u \\in{} \\mathbb{R} $  \n",
    "\n",
    "$ \\hspace{0.9cm} (\\text{i.e. pointwise convergence of characteristic functions}). $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e3989b",
   "metadata": {},
   "source": [
    "#### **(proof)** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b73620",
   "metadata": {},
   "source": [
    "$ \\hspace{0.3cm} \\text{Define } P_{X_{n}}, P_{X}, \\varPhi_{X_{n}}, \\varPhi_{X} \\text{ as follows.} $\n",
    "\n",
    "$$ \\displaystyle{} P_{X_{n}} := \\mathbb{P}\\big({} X_{n} \\in{} \\cdot{} \\big){}, \\;\\; P_{X} := \\mathbb{P}\\big({} X \\in{} \\cdot{} \\big){}, \\;\\; \\varPhi_{X_{n}}(u) := \\mathbb{E}\\big[{} e^{\\mathrm{i} u X_{n}} \\big]{}, \\;\\; \\varPhi_{X}(u) := \\mathbb{E}\\big[{} e^{\\mathrm{i}u X} \\big]{} $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\textbf{Step 1. } (ⅰ) \\Leftrightarrow{} (ⅱ) $\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Interpret convergence in distribution as weak convergence of the induced probability measures.} $\n",
    "\n",
    "$$ \\displaystyle{} X_{n} \\xrightarrow{d.} X \\;\\; \\Leftrightarrow{} \\;\\; P_{X_{n}} \\xrightarrow{w.} P_{X} $$\n",
    "\n",
    "$$ \\text{where } \\, \\xrightarrow{d.} : \\text{convergence in distribution}, \\;\\; \\xrightarrow{w.} : \\text{weak convergence} $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Use the characterization of weak convergence by bounded continuous test functions.} $\n",
    "\n",
    "$$ \\displaystyle{} P_{X_{n}} \\xrightarrow{w.} P_{X} \\;\\; \\Leftrightarrow{} \\;\\; \\forall{} f \\in{} C_{b}(\\mathbb{R}): \\int_{\\mathbb{R}} f(x)\\,P_{X_{n}}(\\mathrm{d}x) \\to{} \\int_{\\mathbb{R}} f(x)\\,P_{X}(\\mathrm{d}x) $$\n",
    "\n",
    "$$ \\text{where } \\, C_{b}(\\mathbb{R}{}) : \\text{bounded continuous functions } f : \\mathbb{R{}} \\to{} \\mathbb{C{}} $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Rewrite the integrals as expectations of } \\, f(X_{n}) \\, \\text{ and } f(X). $\n",
    "\n",
    "$$ \\displaystyle{} \\int_{\\mathbb{R}} f(x)\\,P_{X_{n}}(\\mathrm{d}x) = \\mathbb{E}\\big[{} f(X_{n}) \\big]{}, \\quad \\int_{\\mathbb{R}} f(x)\\,P_{X}(\\mathrm{d}x) = \\mathbb{E}\\big[{} f(X) \\big]{} $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Combine the two displays to obtain condition (ⅱ) from (ⅰ), and conversely.} $\n",
    "\n",
    "$$ \\displaystyle{} X_{n} \\xrightarrow{d.} X \\;\\; \\Leftrightarrow{} \\;\\; \\mathbb{E}\\big[{} f(X_{n}) \\big]{} \\to{} \\mathbb{E}\\big[{} f(X) \\big]{} $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\textbf{Step 2. } (ⅱ) \\Rightarrow{} (ⅲ) $\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Assume (ⅱ) holds and fix } u \\in{} \\mathbb{R}. $\n",
    "\n",
    "$$ \\displaystyle{} \\forall{} f \\in{} C_{b}(\\mathbb{R}): \\mathbb{E}\\big[{} f(X_{n}) \\big]{} \\to{} \\mathbb{E}\\big[{} f(X) \\big]{} $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Choose the bounded continuous test function } f_{u}(x) := e^{\\mathrm{i} u x}, \\, x \\in{} \\mathbb{R}, \\text{ so that } f_{u} \\in{} C_{b}(\\mathbb{R}). $\n",
    "\n",
    "$$ \\displaystyle{} f_{u}(x) := e^{\\mathrm{i} u x}, \\;\\; x \\in{} \\mathbb{R}, \\;\\; f_{u} \\in{} C_{b}(\\mathbb{R}) $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Apply (ⅱ) with } f = f_{u} \\text{ to obtain convergence of the corresponding expectations.} $\n",
    "\n",
    "$$ \\displaystyle{} \\mathbb{E}\\big[{} f_{u}(X_{n}) \\big]{} \\to{} \\mathbb{E}\\big[{} f_{u}(X) \\big]{} $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Identify these expectations with the characteristic functions of } X_{n} \\text{ and } X. $\n",
    "\n",
    "$$ \\displaystyle{} \\mathbb{E}\\big[{} f_{u}(X_{n}) \\big]{} = \\mathbb{E}\\big[{} e^{\\mathrm{i} u X_{n}} \\big]{} = \\varPhi_{X_{n}}(u), \\;\\; \\mathbb{E}\\big[{} f_{u}(X) \\big]{} = \\mathbb{E}\\big[{} e^{\\mathrm{i} u X} \\big]{} = \\varPhi_{X}(u) $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Hence the characteristic functions converge pointwise for every } u \\in{} \\mathbb{R}. $\n",
    "\n",
    "$$ \\displaystyle{} \\forall{} u \\in{} \\mathbb{R}: \\varPhi_{X_{n}}(u) \\to{} \\varPhi_{X}(u) $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Combine the above displays to obtain condition (ⅲ) from (ⅱ)}. $\n",
    "\n",
    "$$ \\mathbb{E}{}\\big[{} f(X_{n}) \\big]{} \\to{} \\mathbb{E}{}\\big[{} f(X) \\big]{} \\;\\; \\Rightarrow{} \\;\\; \\varPhi{}_{X_{n}}(u) \\to{} \\varPhi{}_{X}(u)  $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\textbf{Step 3. } (ⅲ) \\Rightarrow{} (ⅰ) $\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Assume (ⅲ): the characteristic functions converge pointwise for all } u \\in{} \\mathbb{R}. $\n",
    "\n",
    "$$ \\displaystyle{} \\forall{} u \\in{} \\mathbb{R}: \\varPhi_{X_{n}}(u) \\to{} \\varPhi_{X}(u) $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{By L\\'evy continuity theorem, pointwise convergence of characteristic functions implies weak convergence of the laws.} $\n",
    "\n",
    "$$ \\displaystyle{} \\forall{} u \\in{} \\mathbb{R}: \\varPhi_{X_{n}}(u) \\to{} \\varPhi_{X}(u) \\;\\; \\Rightarrow{} P_{X_{n}} \\xrightarrow{w.} P_{X} $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Weak convergence of the laws is equivalent to convergence in distribution of } X_{n} \\text{ to } X. $\n",
    "\n",
    "$$ \\displaystyle{} P_{X_{n}} \\xrightarrow{w.} P_{X} \\;\\; \\Leftrightarrow{} \\;\\; X_{n} \\xrightarrow{d.} X $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Combine the above displays to obtain condition (ⅰ) from (ⅲ)}. $\n",
    "\n",
    "$$ \\displaystyle{} \\forall{} u \\in{} \\mathbb{R}: \\varPhi_{X_{n}}(u) \\to{} \\varPhi_{X}(u) \\;\\; \\Rightarrow{} \\;\\; P_{X_{n}} \\xrightarrow{w.} P_{X} \\;\\; \\Rightarrow{} \\;\\; X_{n} \\xrightarrow{d.} X $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Therefore } (ⅰ), (ⅱ), (ⅲ) \\text{ are all equivalent.} $\n",
    "\n",
    "$$ \\boxed{ X_{n} \\xrightarrow{d.} X \\;\\; \\Leftrightarrow{} \\;\\; \\mathbb{E}[f(X_{n})] \\to{} \\mathbb{E}[f(X)] \\;\\; \\Leftrightarrow{} \\;\\; \\varPhi_{X_{n}}(u) \\to{} \\varPhi_{X}(u) } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a96b38c",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e488d8ff",
   "metadata": {},
   "source": [
    "> ## Q2. **Relations among Types of Convergence**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f69296",
   "metadata": {},
   "source": [
    "#### $ \\text{Prove Proposition } 2.7. $\n",
    "\n",
    "$ \\hspace{0.3cm} \\textbf{PROPOSITION } 2.7. \\text{(Relations among Types of Convergence)} $\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Let } (X_{n}) \\text{ be a sequence of random variables and let } X \\text{ and } Y \\text{ be random variables. Then:} $\n",
    "\n",
    "$ \\hspace{0.3cm} (a) \\, \\text{If } X_{n} \\xrightarrow{\\text{a.s.}} X \\text{ then } X_{n} \\xrightarrow{p} X. $\n",
    "\n",
    "$ \\hspace{0.3cm} (b) \\, \\text{If } X_{n} \\xrightarrow{\\text{m.s.}} X \\text{ then } X_{n} \\xrightarrow{p} X. $\n",
    "\n",
    "$ \\hspace{0.3cm} (c) \\, \\text{If } P\\{ |X_{n}| \\le Y \\} = 1 \\text{ for all } n \\text{ for some fixed random variable } Y \\text{ with } \\mathbb{E}[Y^{2}] < \\infty, \\text{ and if } X_{n} \\xrightarrow{p} X, \\text{ then } X_{n} \\xrightarrow{\\text{m.s.}} X. $\n",
    "\n",
    "$ \\hspace{0.3cm} (d) \\, \\text{If } X_{n} \\xrightarrow{p} X \\text{ then } X_{n} \\xrightarrow{d} X. $\n",
    "\n",
    "$ \\hspace{0.3cm} (e) \\, \\text{Suppose } X_{n} \\to X \\text{ in the } p., \\text{ m.s., or a.s. sense and } X_{n} \\to Y \\text{in the } p., \\text{ m.s., or a.s. sense. Then } P\\{ X = Y \\} = 1. \\text{ That is, if differences on } $\n",
    "\n",
    "$ \\hspace{0.9cm} \\text{sets of probability zero are ignored, a sequence of random variables can have only one limit (if } p., \\text{m.s., and/or a.s. senses are used).} $\n",
    "\n",
    "$ \\hspace{0.3cm} (f) \\, \\text{Suppose } X_{n} \\xrightarrow{d} X \\text{ and } X_{n} \\xrightarrow{d} Y. \\text{ Then } X \\text{ and } Y \\text{ have the same distribution.} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89994ff",
   "metadata": {},
   "source": [
    "#### **(a)** $ \\text{If } X_{n} \\xrightarrow{\\text{a.s.}} X \\text{ then } X_{n} \\xrightarrow{p} X. $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e995a99",
   "metadata": {},
   "source": [
    "$ \\hspace{0.3cm} \\text{Fix } \\varepsilon{} > 0 \\text{ and consider the event where } X_{n} \\text{ deviates from } X \\text{ by more than } \\varepsilon{}. $\n",
    "\n",
    "$$ \\displaystyle{} A_{n}(\\varepsilon{}) := \\big\\{{} \\omega{} \\in{} \\Omega{} : |X_{n}(\\omega{}) - X(\\omega{})| > \\varepsilon{} \\big\\}{} $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Almost sure convergence means } X_{n}(\\omega{}) \\to{} X(\\omega{}) \\text{ for a.e. } \\omega{}, \\text{ hence the indicators converge to } 0 \\text{ a.s.} $\n",
    "\n",
    "$$ \\displaystyle{} \\mathbf{1}_{A_{n}(\\varepsilon{})}(\\omega{}) \\to{} 0 \\;\\; \\text{for a.e.(almost everywhere) } \\omega{} $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Apply dominated convergence to the bounded sequence of indicators } \\mathbf{1}_{A_{n}(\\varepsilon{})} \\leq{} 1. $\n",
    "\n",
    "$$ \\displaystyle{} \\mathbb{E}\\big[{}\\mathbf{1}_{A_{n}(\\varepsilon{})} \\big]{} \\to{} \\mathbb{E}[0] = 0 $$\n",
    "\n",
    "$$ \\displaystyle{} \\text{where } \\mathbf{1}_{(\\cdot{})} : \\text{indicator function of a set } $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Identify the expectation of an indicator with the probability of the underlying event.} $\n",
    "\n",
    "$$ \\displaystyle{} \\mathbb{E} \\big[{} \\mathbf{1}_{A_{n}(\\varepsilon{})} \\big]{}= \\mathbb{P}\\big({}A_{n}(\\varepsilon{}) \\big){}= \\mathbb{P}\\big({}|X_{n} - X| > \\varepsilon{} \\big){}$$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Thus the probability of a deviation larger than } \\varepsilon{} \\text{ goes to } 0, \\text{which is exactly convergence in probability}. $\n",
    "\n",
    "$$ \\forall{} \\varepsilon{} > 0:\\; \\mathbb{P}\\big(|X_{n} - X| > \\varepsilon\\big) \\to 0 \\;\\; \\Leftrightarrow{} \\;\\; X_{n} \\xrightarrow{p} X $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Summarizing the implications, we have} $\n",
    "\n",
    "$$ \\boxed{ X_{n} \\xrightarrow{\\text{a.s.}} X \\;\\; \\Rightarrow{} \\;\\; \\forall{} \\varepsilon > 0:\\, \\mathbb{P}\\big(|X_{n} - X| > \\varepsilon\\big) \\to 0 \\;\\; \\Leftrightarrow{} \\;\\; X_{n} \\xrightarrow{p} X } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba0202e",
   "metadata": {},
   "source": [
    "#### **(b)** $ \\text{If } X_{n} \\xrightarrow{\\text{m.s.}} X \\text{ then } X_{n} \\xrightarrow{p} X. $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3170621",
   "metadata": {},
   "source": [
    "$ \\hspace{0.3cm} \\text{Recall that mean-square convergence means that the } L^{2} \\text{ error goes to zero.} $\n",
    "\n",
    "$$ \\displaystyle{} X_{n} \\xrightarrow{m.s.} X \\;\\Longleftrightarrow{}\\; \\mathbb{E}\\big[{} (X_{n} - X)^{2} \\big]{} \\to{} 0 $$\n",
    "\n",
    "$$ \\text{where } \\xrightarrow{m.s.} : \\text{mean-square convergence}, \\;\\; \\xrightarrow{p} : \\text{convergence in probability}. $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Fix } \\varepsilon > 0 \\text{ and apply Chebyshev's inequality to the nonnegative random variable } (X_{n} - X)^{2}. $\n",
    "\n",
    "$$ \\displaystyle{} \\mathbb{P}\\big({} |X_{n} - X| > \\varepsilon \\big){} = \\mathbb{P}\\big({} (X_{n} - X)^{2} > \\varepsilon^{2} \\big){} \\leq{} \\frac{\\mathbb{E}\\big[{} (X_{n} - X)^{2} \\big]{}}{\\varepsilon^{2}} $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{By mean-square convergence, the numerator } \\mathbb{E}\\big[{} (X_{n} - X)^{2} \\big]{} \\text{ tends to } 0 $\n",
    "\n",
    "$$ \\displaystyle{} \\mathbb{E}\\big[{} (X_{n} - X)^{2} \\big]{} \\to{} 0 \\;\\Rightarrow{}\\; \\mathbb{P}\\big({} |X_{n} - X| > \\varepsilon \\big){} \\to{} 0 \\;\\;\\;\\; \\forall{} \\varepsilon{} > 0 $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Thus the probability of a deviation larger than } \\varepsilon \\text{ goes to zero for every } \\varepsilon > 0, \\text{ i.e. convergence in probability.} $\n",
    "\n",
    "$$ \\displaystyle{} \\boxed{X_{n} \\xrightarrow{m.s.} X \\;\\; \\Rightarrow{} \\;\\; X_{n} \\xrightarrow{p} X} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53479bca",
   "metadata": {},
   "source": [
    "#### **(c)** $ \\text{If } P\\{ |X_{n}| \\le Y \\} = 1 \\text{ for all } n \\text{ for some fixed random variable } Y \\text{ with } \\mathbb{E}[Y^{2}] < \\infty, $\n",
    "#### $ \\hspace{0.45cm} \\text{ and if } X_{n} \\xrightarrow{p} X, \\text{ then } X_{n} \\xrightarrow{\\text{m.s.}} X. $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3afafde",
   "metadata": {},
   "source": [
    "$ \\hspace{0.3cm} \\text{Assume a uniform almost-sure bound by } Y \\text{, a finite second moment of } Y \\text{, and convergence in probability to } X. $\n",
    "\n",
    "$$ \\displaystyle{} |X_{n}| \\le{} Y \\ \\text{a.s. } \\forall{} \\, n, \\;\\; \\mathbb{E}[Y^{2}] < \\infty{}, \\;\\; X_{n} \\xrightarrow{p} X $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Use convergence in probability to extract a subsequence that converges almost surely to } X. $\n",
    "\n",
    "$$ \\displaystyle{} X_{n} \\xrightarrow{p} X \\;\\; \\Rightarrow{} \\;\\; \\exists{} \\, (n_{k}) \\subset{} \\mathbb{N} \\, \\text{ s.t. } \\, X_{n_{k}} \\xrightarrow{a.s.} X $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{On this subsequence, pass the almost-sure bound to the limit to obtain an almost-sure bound for } X. $\n",
    "\n",
    "$$ \\displaystyle{} |X_{n_{k}}| \\le{} Y \\, \\text{ a.s. } \\forall{} k, \\;\\; X_{n_{k}} \\xrightarrow{a.s.} X \\;\\; \\Rightarrow{} \\;\\; |X| \\le{} Y \\;\\; \\text{a.s.} $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Hence both } X_{n} \\text{ and } X \\text{ are dominated by } Y, \\text{ which yields a uniform bound on the squared error.} $\n",
    "\n",
    "$$ \\displaystyle{} |X_{n} - X| \\le{} |X_{n}| + |X| \\le{} 2Y \\;\\; \\Rightarrow{} \\;\\; 0 \\le{} (X_{n} - X)^{2} \\le{} 4Y^{2} \\;\\; \\text{a.s. } \\forall{} \\, n $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{For the subsequence } (X_{n_{k}}), \\text{ the squared error converges almost surely to } 0 \\text{ and is dominated by } 4Y^{2}. $\n",
    "\n",
    "$$ \\displaystyle{} (X_{n_{k}} - X)^{2} \\xrightarrow{a.s.} 0, \\;\\; 0 \\le{} (X_{n_{k}} - X)^{2} \\le{} 4Y^{2}, \\;\\; \\mathbb{E}[Y^{2}] < \\infty{} $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Apply dominated convergence to obtain convergence of the second moments along this subsequence.} $\n",
    "\n",
    "$$ \\displaystyle{} \\mathbb{E}\\big[(X_{n_{k}} - X)^{2}\\big] \\to{} 0 $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Therefore every subsequence of } \\mathbb{E}\\big[(X_{n} - X)^{2}\\big] \\text{ has a further subsequence converging to } 0,\\; \\text{so the whole sequence converges to } 0. $\n",
    "\n",
    "$$ \\displaystyle{} \\mathbb{E}\\big[(X_{n} - X)^{2}\\big] \\to{} 0 \\;\\; \\Rightarrow{} \\;\\; X_{n} \\xrightarrow{m.s.} X $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95adcde3",
   "metadata": {},
   "source": [
    "#### **(d)** $ \\text{If } X_{n} \\xrightarrow{p} X \\text{ then } X_{n} \\xrightarrow{d} X. $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c122a17",
   "metadata": {},
   "source": [
    "$ \\hspace{0.3cm} \\text{Assume convergence in probability and denote the cdfs of } X_{n}, X \\text{ by } F_{n}, F. $\n",
    "\n",
    "$$ F_{n}(x) := \\mathbb{P}(X_{n} \\le{} x), F(x) := \\mathbb{P}(X \\le{} x). $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Fix a continuity point } x \\text{ of } F \\text{ and } \\varepsilon > 0. \\text{ Use simple set inclusions to compare } F_{n}(x) \\text{ and } F. $\n",
    "\n",
    "$$ \\{ X_{n} \\le{} x \\} \\subset \\{ X \\le{} x + \\varepsilon \\} \\cup \\{ |X_{n} - X| > \\varepsilon \\}, \\{ X \\le{} x - \\varepsilon \\} \\subset \\{ X_{n} \\le{} x \\} \\cup \\{ |X_{n} - X| > \\varepsilon \\}. $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Translate these inclusions into upper and lower bounds for } F_{n}(x). $\n",
    "\n",
    "$$ F(x - \\varepsilon) - \\mathbb{P}(|X_{n} - X| > \\varepsilon) \\le{} F_{n}(x) \\le{} F(x + \\varepsilon) + \\mathbb{P}(|X_{n} - X| > \\varepsilon). $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Take } \\liminf \\text{ and } \\limsup \\text{ and use } X_{n} \\xrightarrow{p} X \\text{ so that } \\mathbb{P}(|X_{n} - X| > \\varepsilon) \\to{} 0. $\n",
    "\n",
    "$$ F(x - \\varepsilon) \\le{} \\liminf_{n \\to{} \\infty{}} F_{n}(x) \\le{} \\limsup_{n \\to{} \\infty{}} F_{n}(x) \\le{} F(x + \\varepsilon). $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Let } \\varepsilon \\downarrow 0 \\text{ and use the continuity of } F \\text{ at } x \\text{ to squeeze the limit.} $\n",
    "\n",
    "$$ F(x - \\varepsilon) \\to{} F(x), F(x + \\varepsilon) \\to{} F(x) \\;\\; \\Rightarrow{} \\;\\; \\lim_{n \\to{} \\infty{}} F_{n}(x) = F(x). $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Thus } F_{n}(x) \\to{} F(x) \\text{ for every continuity point } x \\text{ of } F, \\text{ i.e. } X_{n} \\xrightarrow{d} X. $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de2e16b",
   "metadata": {},
   "source": [
    "#### **(e)** $ \\text{Suppose } X_{n} \\to X \\text{ in the } p., \\text{ m.s., or a.s. sense and } X_{n} \\to Y \\text{in the } p., \\text{ m.s., or a.s. sense. Then } P\\{ X = Y \\} = 1. $ \n",
    "#### $ \\hspace{0.45cm} \\text{ That is, if differences on sets of probability zero are ignored, a sequence of random variables can have only} $\n",
    "#### $ \\hspace{0.45cm} \\text{ one limit (if } p., \\text{m.s., and/or a.s. senses are used).} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2ce4fc",
   "metadata": {},
   "source": [
    "$ \\hspace{0.3cm} \\text{By parts (a), (b), (c), a.s. and m.s. convergence imply convergence in probability, so we may assume} $\n",
    "\n",
    "$$ X_{n} \\xrightarrow{p} X, X_{n} \\xrightarrow{p} Y $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Fix } \\varepsilon > 0 \\text{ and use the triangle inequality to relate } |X - Y| \\text{ to } |X - X_{n}| \\text{ and } |X_{n} - Y|. $\n",
    "\n",
    "$$ |X - Y| \\le{} |X - X_{n}| + |X_{n} - Y| \\;\\; \\Rightarrow{} \\;\\; \\{ |X - Y| > \\varepsilon \\} \\subset \\{ |X - X_{n}| > \\varepsilon/2 \\} \\cup \\{ |X_{n} - Y| > \\varepsilon/2 \\} $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Translate this inclusion into a probability bound.} $\n",
    "\n",
    "$$ \\mathbb{P}(|X - Y| > \\varepsilon) \\le{} \\mathbb{P}(|X - X_{n}| > \\varepsilon/2) + \\mathbb{P}(|X_{n} - Y| > \\varepsilon/2) $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Taking } \\limsup_{n \\to{} \\infty{}} \\text{ and using } X_{n} \\xrightarrow{p} X, X_{n} \\xrightarrow{p} Y \\text{ gives} $\n",
    "\n",
    "$$ \\mathbb{P}(|X - Y| > \\varepsilon)\\le{} \\limsup_{n \\to{} \\infty{}} \\mathbb{P}(|X - X_{n}| > \\varepsilon/2)+ \\limsup_{n \\to{} \\infty{}} \\mathbb{P}(|X_{n} - Y| > \\varepsilon/2) = 0 $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Hence } \\mathbb{P}(|X - Y| > \\varepsilon) = 0 \\text{ for every } \\varepsilon > 0. $\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Use the countable family } \\varepsilon_{m} := 1/m \\text{ and the fact that a countable union of null sets is null.} $\n",
    "\n",
    "$$ \\{ |X - Y| > 0 \\} = \\bigcup_{m = 1}^{\\infty{}} \\{ |X - Y| > 1/m \\} \\;\\; \\Rightarrow{} \\;\\; \\mathbb{P}(|X - Y| > 0) \\le{} \\sum_{m = 1}^{\\infty{}} \\mathbb{P}(|X - Y| > 1/m) = 0 $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Therefore } \\mathbb{P}(|X - Y| = 0) = 1, \\text{ i.e. } \\mathbb{P}(X = Y) = 1. $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e1b78d",
   "metadata": {},
   "source": [
    "#### **(f)** $ \\text{Suppose } X_{n} \\xrightarrow{d} X \\text{ and } X_{n} \\xrightarrow{d} Y. \\text{ Then } X \\text{ and } Y \\text{ have the same distribution.} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4960f144",
   "metadata": {},
   "source": [
    "$ \\hspace{0.3cm} \\text{Assume convergence in distribution of } X_{n} \\text{ to both } X \\text{ and } Y. $\n",
    "\n",
    "$$ X_{n} \\xrightarrow{d} X, \\;\\; X_{n} \\xrightarrow{d} Y $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Denote the distribution functions of } X_{n}, X, Y \\text{ by } F_{n}, F_{X}, F_{Y}. $\n",
    "\n",
    "$$ F_{n}(x) := \\mathbb{P}(X_{n} \\le{} x), F_{X}(x) := \\mathbb{P}(X \\le{} x), F_{Y}(x) := \\mathbb{P}(Y \\le{} x) $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{By definition of convergence in distribution, we have pointwise limits at continuity points.} $\n",
    "\n",
    "$$ X_{n} \\xrightarrow{d} X \\;\\; \\Rightarrow{} \\;\\; F_{n}(x) \\to{} F_{X}(x) \\;\\; \\text{ for every continuity point } x \\text{ of } F_{X}, $$\n",
    "\n",
    "$$ X_{n} \\xrightarrow{d} Y \\;\\; \\Rightarrow{} \\;\\; F_{n}(x) \\to{} F_{Y}(x) \\;\\; \\text{ for every continuity point } x \\text{ of } F_{Y} $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Take } x \\text{ which is a continuity point of both } F_{X} \\text{ and } F_{Y} \\text{ (these points form a co-countable set).} $\n",
    "\n",
    "$$ F_{n}(x) \\to{} F_{X}(x), F_{n}(x) \\to{} F_{Y}(x) \\;\\; \\Rightarrow{} \\;\\; F_{X}(x) = F_{Y}(x) $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Hence } F_{X}(x) = F_{Y}(x) \\text{ for all } x \\text{ where both cdfs are continuous.} $\n",
    "\n",
    "$$ F_{X}(x) = F_{Y}(x) \\text{for all continuity points } x \\text{ of } F_{X} \\text{ and } F_{Y} $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Since distribution functions are right-continuous and have at most countably many jumps,} $\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{equality on all continuity points implies } F_{X}(x) = F_{Y}(x) \\text{ for every } x \\in \\mathbb{R}. $\n",
    "\n",
    "$$ F_{X}(x) = F_{Y}(x) \\forall x \\in \\mathbb{R} $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Equality of distribution functions means that } X \\text{ and } Y \\text{ have the same distribution.} $\n",
    "\n",
    "$$ \\boxed{X \\stackrel{d}{=} Y} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eef1f87",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc92454d",
   "metadata": {},
   "source": [
    "> ## Q3. **Sums of i.i.d. random variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb15aae",
   "metadata": {},
   "source": [
    "#### $ \\text{Problem } 2.21. \\text{(Sums of i.i.d. random variables, III)} $\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Fix } \\lambda > 0. \\text{ For each integer } n > \\lambda, \\text{ let } X_{1,n}, X_{2,n}, \\ldots, X_{n,n} \\text{ be independent random variables such that}$  \n",
    "\n",
    "$ \\hspace{0.3cm} P[X_{i,n} = 1] = \\lambda / n \\text{ and } P[X_{i,n} = 0] = 1 - (\\lambda / n). \\text{ Let } Y_{n} = X_{1,n} + X_{2,n} + \\cdots + X_{n,n}. $\n",
    "\n",
    "$ \\hspace{0.3cm} (a)\\; \\text{Compute the characteristic function of } Y_{n} \\text{ for each } n. $\n",
    "\n",
    "$ \\hspace{0.3cm} (b)\\; \\text{Find the pointwise limit of the characteristic functions as } n \\to \\infty. $\n",
    "\n",
    "$ \\hspace{0.75cm} \\text{The limit is the characteristic function of what probability distribution?} $\n",
    "\n",
    "$ \\hspace{0.3cm} (c)\\; \\text{In what sense(s), if any, does the sequence } (Y_{n}) \\text{ converge?} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef73d94e",
   "metadata": {},
   "source": [
    "#### **(a)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5389d41d",
   "metadata": {},
   "source": [
    "$\\hspace{0.3cm} \\text{For each } i \\in \\{1,\\dots,n\\}, \\; X_{i,n} \\text{ is a Bernoulli random variable with parameter } p_{n} = \\lambda/n. $  \n",
    "$\\hspace{0.3cm} \\text{By the definition of the characteristic function, first compute } \\varPhi_{X_{i,n}}(t). $\n",
    "$$ \\begin{aligned} \\varPhi_{X_{i,n}}(t) &:= \\mathbb{E}\\!\\left[e^{it X_{i,n}}\\right] \\\\ &= e^{it \\cdot 1} \\, \\mathbb{P}(X_{i,n} = 1) + e^{it \\cdot 0} \\, \\mathbb{P}(X_{i,n} = 0) \\\\ &= e^{it} \\cdot \\frac{\\lambda}{n} + 1 \\cdot \\Big(1 - \\frac{\\lambda}{n}\\Big) \\\\ &= 1 + \\frac{\\lambda}{n}\\big(e^{it} - 1\\big). \\end{aligned} $$\n",
    "$\\hspace{0.3cm} \\text{Now } Y_{n} = \\sum_{i=1}^{n} X_{i,n} \\text{ is the sum of independent random variables, so its characteristic function is the product of the individual ones.} $\n",
    "$$ \\begin{aligned} \\varPhi_{Y_{n}}(t) &= \\mathbb{E}\\!\\left[e^{it Y_{n}}\\right] = \\mathbb{E}\\!\\left[e^{it \\sum_{i=1}^{n} X_{i,n}}\\right] \\\\ &= \\mathbb{E}\\!\\left[\\prod_{i=1}^{n} e^{it X_{i,n}}\\right] = \\prod_{i=1}^{n} \\mathbb{E}\\!\\left[e^{it X_{i,n}}\\right] \\quad (\\text{by independence}) \\\\ &= \\left(\\varPhi_{X_{1,n}}(t)\\right)^{n} \\\\ &= \\left(1 + \\frac{\\lambda}{n}\\big(e^{it} - 1\\big)\\right)^{n}. \\end{aligned} $$\n",
    "$\\hspace{0.3cm} \\text{Thus for each } n, \\text{ the characteristic function of } Y_{n} \\text{ is} $\n",
    "$$ \\displaystyle \\boxed{\\varPhi_{Y_{n}}(t) = \\left(1 + \\frac{\\lambda}{n}\\big(e^{it} - 1\\big)\\right)^{n}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdca2c54",
   "metadata": {},
   "source": [
    "#### **(b)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710f3ccf",
   "metadata": {},
   "source": [
    "\n",
    "$\\hspace{0.3cm} \\text{From part (a), we have} $\n",
    "$$ \\varPhi_{Y_{n}}(t) = \\left(1 + \\frac{\\lambda}{n}\\big(e^{it} - 1\\big)\\right)^{n}. $$\n",
    "$\\hspace{0.3cm} \\text{Recall the standard limit } \\displaystyle \\lim_{n \\to \\infty} \\Big(1 + \\frac{z}{n}\\Big)^{n} = e^{z}. \\text{ Take } z = \\lambda\\big(e^{it} - 1\\big). $\n",
    "$$ \\begin{aligned} \\lim_{n \\to \\infty} \\varPhi_{Y_{n}}(t) &= \\lim_{n \\to \\infty} \\left(1 + \\frac{\\lambda}{n}\\big(e^{it} - 1\\big)\\right)^{n} \\\\ &= \\exp\\!\\left(\\lambda\\big(e^{it} - 1\\big)\\right). \\end{aligned} $$\n",
    "$\\hspace{0.3cm} \\text{Next, identify this limit as the characteristic function of a familiar distribution.} $  \n",
    "$\\hspace{0.3cm} \\text{Let } N \\sim \\mathrm{Poisson}(\\lambda). \\text{ Then its probability mass function is} $\n",
    "$$ \\mathbb{P}(N = k) = e^{-\\lambda}\\frac{\\lambda^{k}}{k!}, \\quad k = 0,1,2,\\dots $$\n",
    "$\\hspace{0.3cm} \\text{Compute the characteristic function of } N. $\n",
    "$$ \\begin{aligned} \\varPhi_{N}(t) &= \\mathbb{E}\\!\\left[e^{itN}\\right] = \\sum_{k=0}^{\\infty} e^{itk} \\, \\mathbb{P}(N = k) \\\\ &= \\sum_{k=0}^{\\infty} e^{itk} \\, e^{-\\lambda} \\frac{\\lambda^{k}}{k!} \\\\ &= e^{-\\lambda} \\sum_{k=0}^{\\infty} \\frac{(\\lambda e^{it})^{k}}{k!} \\\\ &= e^{-\\lambda} \\exp\\!\\big(\\lambda e^{it}\\big) \\\\ &= \\exp\\!\\big(\\lambda e^{it} - \\lambda\\big) = \\exp\\!\\left(\\lambda\\big(e^{it} - 1\\big)\\right). \\end{aligned} $$\n",
    "$\\hspace{0.3cm} \\text{Therefore } \\exp\\!\\left(\\lambda\\big(e^{it} - 1\\big)\\right) \\text{ is exactly the characteristic function of } \\mathrm{Poisson}(\\lambda). $\n",
    "$$ \\displaystyle \\boxed{\\lim_{n \\to \\infty} \\varPhi_{Y_{n}}(t) = \\exp\\!\\left(\\lambda\\big(e^{it} - 1\\big)\\right) = \\varPhi_{\\mathrm{Poisson}(\\lambda)}(t)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561352d4",
   "metadata": {},
   "source": [
    "#### **(c)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a50f01",
   "metadata": {},
   "source": [
    "$\\hspace{0.3cm} \\text{From part (b), for each } t \\in \\mathbb{R}, $\n",
    "$$ \\varPhi_{Y_{n}}(t) = \\left(1 + \\frac{\\lambda}{n}(e^{it} - 1)\\right)^{n} \\;\\longrightarrow\\; \\exp\\!\\left(\\lambda(e^{it} - 1)\\right) =: \\varPhi_{N}(t). $$\n",
    "\n",
    "$\\hspace{0.3cm} \\text{Let } N \\sim \\mathrm{Poisson}(\\lambda). \\text{ Then} $\n",
    "$$ \\varPhi_{N}(t) = \\mathbb{E}[e^{itN}] = \\exp\\!\\left(\\lambda(e^{it} - 1)\\right), \\quad t \\in \\mathbb{R}. $$\n",
    "\n",
    "$\\hspace{0.3cm} \\text{Hence } \\varPhi_{Y_{n}}(t) \\to \\varPhi_{N}(t) \\text{ pointwise and } \\varPhi_{N} \\text{ is a characteristic function. By L\\'evy’s continuity theorem,} $\n",
    "$$ Y_{n} \\xrightarrow{d} N \\quad \\text{with } N \\sim \\mathrm{Poisson}(\\lambda). $$\n",
    "\n",
    "$\\hspace{0.3cm} \\text{Therefore, the sequence } (Y_{n}) \\text{ converges in distribution to } \\mathrm{Poisson}(\\lambda). $\n",
    "$\\hspace{0.3cm} \\text{From the given information, no convergence in probability, mean square, or almost surely can be concluded.} $\n",
    "$$ \\displaystyle \\boxed{Y_{n} \\xrightarrow{d} \\mathrm{Poisson}(\\lambda) \\quad \\text{(no stronger mode of convergence is implied).}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c9077b",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4995949",
   "metadata": {},
   "source": [
    "> ## Q4. **_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5708725b",
   "metadata": {},
   "source": [
    "#### $ \\text{Problem } 2.25. \\text{(Limit behavior of a stochastic dynamical system)} $\n",
    "\n",
    "$\\hspace{0.3cm} \\text{Let } W_{1}, W_{2}, \\ldots \\text{ be a sequence of independent, } N(0, 0.5) \\text{ random variables. Let } X_{0} = 0,$  \n",
    "$\\hspace{0.3cm} \\text{and define } X_{1}, X_{2}, \\ldots \\text{ recursively by } X_{k+1} = X_{k}^{2} + W_{k}. $  \n",
    "\n",
    "$\\hspace{0.3cm} \\text{Determine in which of the senses (a.s., m.s., p., d.) the sequence } (X_{n}) \\text{ converges as } n \\to \\infty,$  \n",
    "$\\hspace{0.3cm} \\text{and identify the limit, if any. Justify your answer.} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4908bd9",
   "metadata": {},
   "source": [
    "#### **(proof)** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42da969",
   "metadata": {},
   "source": [
    "$ \\hspace{0.3cm} \\textbf{Step 1. Basic observation about the dynamics.} $\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{The recursion is } X_{k+1} = X_{k}^{2} + W_{k}, \\text{ where } W_{k} \\sim N(0,0.5) \\text{ are independent.}$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{The noise } W_{k} \\text{ has unbounded support, so for any } a > 0 \\text{ we have } \\mathbb{P}(W_{k} > a) > 0.$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Once } X_{k} \\text{ becomes moderately large and positive, the deterministic map } x \\mapsto x^{2} \\text{ makes the state blow up very fast.}$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{For example, if from some time } n \\text{ on we ignore the noise and start at } X_{n} = 3, \\text{ then} $\n",
    "\n",
    "$$ 3 \\;\\mapsto\\; 9 \\;\\mapsto\\; 81 \\;\\mapsto\\; 6561 \\;\\mapsto\\; 43046721 \\;\\mapsto\\; \\cdots $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{The Gaussian noise is relatively small compared to } X_{k}^{2} \\text{ when } X_{k} \\text{ is large, so it cannot prevent the growth.} $\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{The textbook shows (via a careful construction of nice divergence events) that} $\n",
    "\n",
    "$$ \\mathbb{P}\\big\\{\\lim_{n \\to \\infty{}} X_{n} = +\\infty{} \\big\\} = 1. $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{We take this almost sure blow-up as a known fact and now analyze the types of convergence using only definitions.} $\n",
    "\n",
    "$ \\hspace{0.3cm} \\textbf{Step 2. Consequences of } \\lim_{n \\to \\infty{}} X_{n} = +\\infty{} \\text{ a.s.} $\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Fix an arbitrary } M \\in \\mathbb{R}. \\text{ On the event } \\{\\lim_{n} X_{n} = +\\infty{}\\} \\text{ we have} $\n",
    "\n",
    "$$ \\forall M \\in \\mathbb{R} \\;\\;\\exists N(\\omega) \\text{ such that } X_{n}(\\omega) > M \\text{ for all } n \\ge N(\\omega). $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Equivalently, for each fixed } M \\text{ the indicators satisfy} $\n",
    "\n",
    "$$ \\mathbf{1}_{\\{X_{n} > M\\}}(\\omega) \\;\\xrightarrow[n \\to \\infty{}]{}\\; 1 \\quad \\text{for a.e. } \\omega. $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{The indicators are bounded by } 0 \\le \\mathbf{1}_{\\{X_{n} > M\\}} \\le 1, \\text{ so we can apply dominated convergence:} $\n",
    "\n",
    "$$ \\mathbb{P}(X_{n} > M) = \\mathbb{E}\\big[\\mathbf{1}_{\\{X_{n} > M\\}}\\big] \\;\\xrightarrow[n \\to \\infty{}]{}\\; \\mathbb{E}[1] = 1. $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Thus for every fixed } M \\in \\mathbb{R} \\text{ we have} $\n",
    "\n",
    "$$ \\lim_{n \\to \\infty{}} \\mathbb{P}(X_{n} > M) = 1, \\qquad \\lim_{n \\to \\infty{}} \\mathbb{P}(|X_{n}| \\le M) = 0. $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{By definition, this is convergence in probability to } +\\infty{}: $\n",
    "\n",
    "$$ X_{n} \\xrightarrow{p} +\\infty{}. $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{In particular, there is no finite-valued random variable } X \\text{ such that } X_{n} \\xrightarrow{p} X, \\text{ because} $\n",
    "\n",
    "$$ \\mathbb{P}(|X_{n}| \\le M) \\to 0 \\quad \\text{for every } M, \\quad \\text{but for any finite } X \\text{ we have } \\mathbb{P}(|X| \\le M) \\to 1 \\text{ as } M \\to \\infty{}. $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Hence there is no convergence in probability to any ordinary (finite) random variable. The only possible limit is } +\\infty{}. $\n",
    "\n",
    "$ \\hspace{0.3cm} \\textbf{Step 3. Convergence in distribution to } +\\infty{}. $\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{For each } x \\in \\mathbb{R}, \\text{ the distribution function of } X_{n} \\text{ is } F_{X_{n}}(x) = \\mathbb{P}(X_{n} \\le x).$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{From Step 2 with } M = x \\text{ we have } \\mathbb{P}(X_{n} > x) \\to 1, \\text{ so} $\n",
    "\n",
    "$$ F_{X_{n}}(x) = \\mathbb{P}(X_{n} \\le x) = 1 - \\mathbb{P}(X_{n} > x) \\;\\xrightarrow[n \\to \\infty{}]{}\\; 0 \\quad \\text{for every finite } x. $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Thus, the mass of the distribution escapes to } +\\infty{}: $\n",
    "\n",
    "$$ \\forall x \\in \\mathbb{R} : \\quad \\lim_{n \\to \\infty{}} F_{X_{n}}(x) = 0. $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{In the extended sense we can write} $\n",
    "\n",
    "$$ X_{n} \\xrightarrow{d} +\\infty{}. $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{However, there is no ordinary probability distribution on } \\mathbb{R} \\text{ with distribution function } F(x) \\equiv 0,$\n",
    "$ \\hspace{0.3cm} \\text{because a genuine CDF must satisfy } \\lim_{x \\to +\\infty{}} F(x) = 1.$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{So } (X_{n}) \\text{ does not converge in distribution to any finite-valued random variable.}$\n",
    "\n",
    "$ \\hspace{0.3cm} \\textbf{Step 4. Almost sure and mean-square convergence to an ordinary random variable.} $\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{We already know} \\; \\mathbb{P}\\{\\lim_{n} X_{n} = +\\infty{}\\} = 1. \\text{ So there is no finite random variable } X \\text{ with} $\n",
    "\n",
    "$$ \\mathbb{P}\\Big\\{\\lim_{n \\to \\infty{}} X_{n} = X \\Big\\} = 1, $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{because on the event } \\{\\lim_{n} X_{n} = +\\infty{}\\} \\text{ the values } X_{n}(\\omega) \\text{ are unbounded, while any finite } X(\\omega) \\text{ is bounded.}$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Therefore there is no almost sure convergence to an ordinary random variable.}$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{From Proposition 2.7 in the notes, we know} $\n",
    "\n",
    "$$ X_{n} \\xrightarrow{\\text{m.s.}} X \\;\\Longrightarrow\\; X_{n} \\xrightarrow{p} X. $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Since we have already seen that there is no finite } X \\text{ such that } X_{n} \\xrightarrow{p} X, $\n",
    "$ \\hspace{0.3cm} \\text{it follows that there is no mean-square limit to a finite random variable either.}$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{Moreover, convergence in mean-square to } +\\infty{} \\text{ is not defined, because } \\mathbb{E}[|+\\infty{}|^{2}] \\text{ is not finite.} $\n",
    "\n",
    "$ \\hspace{0.3cm} \\textbf{Step 5. Summary of the limit behavior.} $\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{In the extended sense (allowing } +\\infty{} \\text{ as a limit value), we have} $\n",
    "\n",
    "$$ \\displaystyle X_{n} \\xrightarrow{\\text{a.s.}} +\\infty{}, \\qquad X_{n} \\xrightarrow{p} +\\infty{}, \\qquad X_{n} \\xrightarrow{d} +\\infty{}. $$\n",
    "\n",
    "$ \\hspace{0.3cm} \\text{However, if we restrict attention to ordinary (finite-valued) random variables, then } (X_{n}) $\n",
    "$ \\hspace{0.3cm} \\text{does not converge in the a.s., m.s., p., or d. sense to any finite random variable. In particular, there is no mean-square limit.} $\n",
    "\n",
    "$$ \\boxed{\\displaystyle \\text{There is no convergence to any ordinary random variable in the a.s., m.s., p., or d. senses.}} $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PYTCH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
